#!/usr/bin/env python3
"""
Multi-Agent Configuration
Integrates multiple specialized person agents into the LiveKit voice agent
"""

import logging
from typing import Literal, Optional
from pydantic import BaseModel, Field

from livekit.agents import llm
from livekit.agents.voice import Agent
from livekit.agents.llm import function_tool
from livekit.agents import AutoSubscribe
from livekit.plugins import silero

# Import multi-agent RAG service
import sys
from pathlib import Path

# Add parent directory to path
backend_src = Path(__file__).parent.parent
if str(backend_src) not in sys.path:
    sys.path.insert(0, str(backend_src))

from rag.multi_agent_rag_service import get_multi_agent_rag_service

logger = logging.getLogger(__name__)


# ============================================================================
# Pydantic Configuration Models (keeping existing structure)
# ============================================================================

class STTConfig(BaseModel):
    """STT (Speech-to-Text) Configuration"""
    
    model: str = Field(
        default="assemblyai/universal-streaming",
        description="STT model identifier"
    )


class LLMConfig(BaseModel):
    """LLM (Large Language Model) Configuration"""
    
    model: str = Field(
        default="openai/gpt-4o-mini",
        description="LLM model identifier"
    )
    
    instructions: str = Field(
        default="""You are part of a networking event simulation with multiple specialized experts.
        
Your role is to facilitate natural conversations and route questions to the appropriate expert.
When a user asks a question, you will have access to specialized person agents who each have deep knowledge in their specific topic area.

Key guidelines:
- Be natural and conversational as if you're at a real networking event
- Each expert has their own personality and backstory - respect that
- When answering questions, incorporate the context provided from the expert's knowledge base
- Handle handoffs smoothly when redirecting to another expert
- Stay in character with the current expert's personality
""",
        description="System instructions for multi-agent coordination"
    )


class TTSConfig(BaseModel):
    """TTS (Text-to-Speech) Configuration"""
    
    model: str = Field(
        default="cartesia/sonic-3:a167e0f3-df7e-4d52-a9c3-f949145efdab",
        description="TTS model identifier"
    )


class VADConfig(BaseModel):
    """VAD (Voice Activity Detection) Configuration"""
    
    min_speech_duration: float = Field(default=0.1, ge=0.05, le=1.0)
    min_silence_duration: float = Field(default=0.6, ge=0.1, le=2.0)
    activation_threshold: float = Field(default=0.5, ge=0.0, le=1.0)


class SessionConfig(BaseModel):
    """Agent Session Configuration"""
    
    turn_detection: Literal["vad", "stt", "realtime_llm", "manual"] = Field(default="vad")
    preemptive_generation: bool = Field(default=True)
    allow_interruptions: bool = Field(default=True)
    min_endpointing_delay: float = Field(default=0.5, ge=0.0)
    max_endpointing_delay: float = Field(default=3.0, ge=0.0)


class AgentConfig(BaseModel):
    """Complete Agent Configuration"""
    
    stt: STTConfig = Field(default_factory=STTConfig)
    llm: LLMConfig = Field(default_factory=LLMConfig)
    tts: TTSConfig = Field(default_factory=TTSConfig)
    vad: VADConfig = Field(default_factory=VADConfig)
    session: SessionConfig = Field(default_factory=SessionConfig)
    greeting_message: str = Field(
        default="",  # Will be dynamically generated by orchestrator
        description="Initial greeting (auto-generated by multi-agent system)"
    )


# Default configuration
default_config = AgentConfig()


# ============================================================================
# Multi-Agent Integration
# ============================================================================

def create_agent(config: AgentConfig | None = None) -> Agent:
    """
    Create and configure the LiveKit voice agent with multi-agent orchestration.
    
    The agent integrates multiple specialized person agents, each with:
    - Unique personality and backstory
    - Specialized topic knowledge (from separate FAISS collections)
    - Ability to hand off conversations to other experts
    
    Args:
        config: Optional AgentConfig instance. If None, uses default_config.
    
    Returns:
        Agent: Fully configured agent with multi-agent orchestration
    """
    if config is None:
        config = default_config
    
    # Initialize multi-agent RAG service
    logger.info("Initializing Multi-Agent RAG service...")
    try:
        multi_agent_service = get_multi_agent_rag_service()
        logger.info("âœ… Multi-Agent RAG service initialized")
        
        # Log all available agents
        people = multi_agent_service.get_all_people()
        logger.info(f"Available experts ({len(people)}):")
        for person in people:
            logger.info(f"  - {person.name}: {person.topic} ({person.personality_type})")
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Multi-Agent RAG service: {e}")
        raise
    
    # Create chat context with a single generic system prompt
    chat_ctx = llm.ChatContext()
    system_prompt = (
        "You are facilitating a voice-agent networking mixer featuring three specialists:\n"
        "- Skye Morales (interruption)\n"
        "- Noah Reed (latency)\n"
        "- Avery Kim (streaming)\n\n"
        "The event stays focused on interruption management, latency, and streaming for live voice experiences. "
        "Handle light conversation yourself, but whenever the user asks anything technical or summons an expert, "
        "call the `query_networking_event` tool. Read the tool's response verbatimâ€”it already includes "
        "the speaker's backstory, personality, and topic-specific context.\n\n"
        "IMPORTANT: When the tool returns a handoff message (indicating a switch to another expert), "
        "you MUST speak that message exactly as provided. Do not paraphrase or add commentaryâ€”just say the message verbatim."
    )
    chat_ctx.add_message(role="system", content=system_prompt)
    
    # Configure VAD model
    vad_model = silero.VAD.load(
        min_speech_duration=config.vad.min_speech_duration,
        min_silence_duration=config.vad.min_silence_duration,
        activation_threshold=config.vad.activation_threshold,
    )
    
    # Store LLM model string for use in message generation
    llm_model_str = config.llm.model
    
    # Define the multi-agent query function tool
    @function_tool(
        name="query_networking_event",
        description="Query the networking event to get information from specialized experts. Use this when users ask questions about any topic - the system will automatically route to the right expert and handle handoffs between experts."
    )
    async def query_networking_event(query: str) -> str:
        """
        Query the multi-agent networking event system.
        
        Args:
            query: The user's question
        
        Returns:
            Response from the appropriate expert with context
        """
        try:
            # Get response from orchestrator
            result = multi_agent_service.query(query)
            
            if result["handoff"]:
                # Handoff occurred - split into two parts: transition then greeting
                logger.info(f"ðŸ”„ Handoff: switching to {result['person']}")
                
                # Get session and context from agent
                session = getattr(agent, '_session', None)
                ctx = getattr(agent, '_ctx', None)
                
                # Get new person identity
                new_person = result["person_obj"]
                old_person = result.get("old_person")
                new_identity = new_person.name
                
                # Generate context-aware transition and greeting messages using LLM
                transition_message, greeting_message = await _generate_context_aware_handoff_messages(
                    llm_model_str=llm_model_str,
                    old_person=old_person,
                    new_person=new_person,
                    user_query=result.get("user_query", query),
                    conversation_summary=result.get("conversation_summary", ""),
                    fallback_transition=result.get("transition_message", ""),
                    fallback_greeting=result.get("greeting_message", ""),
                )
                
                # Step 1: Return transition message - current person says this
                logger.info(f"âœ… Returning transition message for current person: {transition_message[:100]}...")
                
                # Step 2: Schedule greeting to be spoken after transition
                # We'll use a background task to send data message and speak greeting
                async def handle_handoff_greeting():
                    """Handle the greeting part of handoff after transition is spoken"""
                    import asyncio
                    # Calculate delay based on message length (roughly 150 words per minute = 2.5 words per second)
                    # Add extra buffer for TTS processing
                    word_count = len(transition_message.split())
                    estimated_seconds = max(2.0, (word_count / 2.5) + 0.5)  # At least 2 seconds, plus buffer
                    logger.info(f"Waiting {estimated_seconds:.1f}s for transition message to finish (estimated from {word_count} words)")
                    await asyncio.sleep(estimated_seconds)
                    
                    # Send data message with new person name and details
                    if ctx:
                        try:
                            import json
                            from livekit import rtc
                            person_data = {
                                "type": "person_name",
                                "name": new_identity
                            }
                            # Add person details
                            if new_person:
                                person_data.update({
                                    "backstory": new_person.backstory,
                                    "topic": new_person.topic,
                                    "personality": new_person.personality_type
                                })
                            person_name_data = json.dumps(person_data)
                            data_bytes = person_name_data.encode('utf-8')
                            
                            try:
                                await ctx.room.local_participant.publish_data(
                                    data_bytes,
                                    kind=rtc.DataPacket_Kind.RELIABLE
                                )
                                logger.info(f"âœ… Sent person name data message after transition: {new_identity}")
                            except (TypeError, AttributeError):
                                await ctx.room.local_participant.publish_data(data_bytes)
                                logger.info(f"âœ… Sent person name data message after transition (method 2): {new_identity}")
                            
                            # Small delay to ensure data message is processed
                            await asyncio.sleep(0.1)
                        except Exception as e:
                            logger.warning(f"Failed to send person name data message: {e}")
                    
                    # Speak the greeting from new person
                    if session and greeting_message:
                        try:
                            logger.info(f"âœ… Speaking greeting from new person: {greeting_message[:100]}...")
                            await session.say(greeting_message, allow_interruptions=True)
                        except Exception as e:
                            logger.warning(f"Failed to speak greeting message: {e}")
                
                # Start background task for greeting
                if session:
                    import asyncio
                    asyncio.create_task(handle_handoff_greeting())
                
                # Return transition message - this will be spoken by current person
                return transition_message.strip()
            else:
                # Current person handles query
                person_obj = result["person_obj"]
                context = result["context"]
                
                persona_prompt = result.get("system_prompt", "")
                
                rag_context = context or "[No context retrieved]"
                response = f"""{persona_prompt}

[Context from {person_obj.name}'s knowledge base]
{rag_context}

[Instructions]
Using the context above, answer the user's question: "{query}"
Stay in character as {person_obj.name} with a {person_obj.personality_type} personality.
Make sure your tone reflects their backstory (e.g., Skye cracks jokes every time, Avery is curt and a little rude).
Integrate the context naturally without saying "the context says" or similar.
"""
                logger.info(
                    f"âœ… {person_obj.name} retrieved context ({len(context or '')} chars)"
                )
                return response
        
        except Exception as e:
            logger.error(f"âŒ Error querying multi-agent system: {e}")
            active_person = None
            excuse = (
                "Everyone's juggling another conversation right now."
            )
            try:
                active_person = multi_agent_service.get_current_person()
            except Exception:
                active_person = None
            if active_person:
                excuse = (
                    f"{active_person.name} just stepped away to help someone else"
                    "â€”probably ran to grab water or take a quick break."
                )
            return (
                f"{excuse} Give them a moment and feel free to ask again or"
                " check in with another expert."
            )
    
    # Create Agent with multi-agent tool
    agent = Agent(
        instructions=system_prompt,
        chat_ctx=chat_ctx,
        stt=config.stt.model,
        llm=config.llm.model,
        tts=config.tts.model,
        vad=vad_model,
        tools=[query_networking_event],
    )
    
    # Store multi-agent service on agent for reference
    agent._multi_agent_service = multi_agent_service
    logger.info("âœ… Multi-agent system integrated with LiveKit agent")
    
    return agent


async def _generate_context_aware_handoff_messages(
    llm_model_str: str,
    old_person,
    new_person,
    user_query: str,
    conversation_summary: str,
    fallback_transition: str,
    fallback_greeting: str,
) -> tuple[str, str]:
    """
    Generate context-aware transition and greeting messages using LLM.
    
    Args:
        llm_model_str: LLM model identifier string
        old_person: PersonAgent being handed off from
        new_person: PersonAgent being handed off to
        user_query: The user's query that triggered the handoff
        conversation_summary: Summary of conversation history
        fallback_transition: Fallback transition message if LLM generation fails
        fallback_greeting: Fallback greeting message if LLM generation fails
    
    Returns:
        Tuple of (transition_message, greeting_message)
    """
    try:
        # Create LLM instance from model string
        from livekit.agents import llm as llm_module
        try:
            llm_model = llm_module.LLM.create(llm_model_str)
        except Exception as e:
            logger.warning(f"Failed to create LLM instance: {e}, using fallback")
            return fallback_transition, fallback_greeting
        
        # Create a temporary chat context for generating messages
        temp_ctx = llm_module.ChatContext()
        
        # Build prompt for transition message (from old person's perspective)
        transition_prompt = f"""You are {old_person.name} at a networking event. You need to hand off the conversation to {new_person.name}, who specializes in {new_person.topic.replace('_', ' ')}.

CONVERSATION CONTEXT:
{conversation_summary if conversation_summary and conversation_summary.strip() != "No conversation history yet." else "This is the beginning of the conversation."}

USER'S REQUEST:
"{user_query}"

YOUR PERSONALITY:
{old_person.personality.description}
{old_person.personality.response_style}

YOUR BACKSTORY:
{old_person.backstory}

TASK:
Generate a natural, in-character transition message (1-2 sentences) where you introduce {new_person.name} to help with the user's question. 
- Stay true to your personality ({old_person.personality_type})
- Reference the conversation naturally if relevant
- Be authentic and conversational, as if you're at a real networking event
- Keep it brief and natural

Generate ONLY the transition message, nothing else:"""

        temp_ctx.add_message(role="system", content=transition_prompt)
        temp_ctx.add_message(role="user", content="Generate the transition message now.")
        
        # Generate transition message
        transition_response = await llm_model.chat(ctx=temp_ctx)
        transition_message = transition_response.choices[0].message.content.strip() if transition_response.choices else fallback_transition
        
        # Clear context for greeting
        temp_ctx = llm_module.ChatContext()
        
        # Build prompt for greeting message (from new person's perspective)
        greeting_prompt = f"""You are {new_person.name} at a networking event. You've just been introduced to help with a question about {new_person.topic.replace('_', ' ')}.

CONVERSATION CONTEXT:
{conversation_summary if conversation_summary and conversation_summary.strip() != "No conversation history yet." else "This is the beginning of the conversation."}

USER'S REQUEST:
"{user_query}"

YOUR PERSONALITY:
{new_person.personality.description}
{new_person.personality.response_style}

YOUR BACKSTORY:
{new_person.backstory}

TASK:
Generate a natural, in-character greeting message (1-2 sentences) where you introduce yourself and acknowledge you can help with their question.
- Stay true to your personality ({new_person.personality_type})
- Reference the conversation context naturally if relevant
- Be authentic and conversational
- Keep it brief and welcoming

Generate ONLY the greeting message, nothing else:"""

        temp_ctx.add_message(role="system", content=greeting_prompt)
        temp_ctx.add_message(role="user", content="Generate the greeting message now.")
        
        # Generate greeting message
        greeting_response = await llm_model.chat(ctx=temp_ctx)
        greeting_message = greeting_response.choices[0].message.content.strip() if greeting_response.choices else fallback_greeting
        
        logger.info(f"âœ… Generated context-aware messages:")
        logger.info(f"   Transition: {transition_message[:100]}...")
        logger.info(f"   Greeting: {greeting_message[:100]}...")
        
        return transition_message, greeting_message
        
    except Exception as e:
        logger.warning(f"Failed to generate context-aware messages: {e}, using fallback")
        return fallback_transition, fallback_greeting


def get_greeting_message(config: AgentConfig | None = None) -> str:
    """
    Get the initial greeting from the randomly selected person at the networking event.
    
    Args:
        config: Optional AgentConfig instance
    
    Returns:
        str: Personalized greeting from current person
    """
    try:
        multi_agent_service = get_multi_agent_rag_service()
        greeting = multi_agent_service.get_initial_greeting()
        return greeting
    except Exception as e:
        logger.error(f"Error getting greeting: {e}")
        return "Hello! Welcome to the networking event. How can I help you today?"